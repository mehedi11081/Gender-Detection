{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1a3448",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9292253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e0e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3915e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = [f for f in glob.glob(r'C:\\Users\\ii\\Jupyter-Notebook\\gender-detection\\gender_dataset_face'+'\\**\\*', recursive=True) if not os.path.isdir(f)]\n",
    "random.shuffle(image_files)\n",
    "\n",
    "#print(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55790fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in image_files:\n",
    "    image = cv2.imread(img)\n",
    "    image = cv2.resize(image, (96,96))\n",
    "    image = img_to_array(image)\n",
    "    data.append(image)\n",
    "    label = img.split(os.path.sep)[-2]\n",
    "    if label == \"woman\":\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "        \n",
    "    labels.append([label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "550fb255",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b177f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2,\n",
    "                                                  random_state=42)\n",
    "\n",
    "trainY = to_categorical(trainY, num_classes=2)\n",
    "testY = to_categorical(testY, num_classes=2)\n",
    "\n",
    "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aad1a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(width, height, depth, classes):\n",
    "    model = Sequential()\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        inputShape = (depth, height, width)\n",
    "        chanDim = 1\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=inputShape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fd3b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ii\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ii\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "28/28 [==============================] - 48s 1s/step - loss: 0.7508 - accuracy: 0.7131 - val_loss: 0.6526 - val_accuracy: 0.6234\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.5435 - accuracy: 0.7996 - val_loss: 0.7025 - val_accuracy: 0.5541\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.4253 - accuracy: 0.8226 - val_loss: 1.7278 - val_accuracy: 0.5195\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.3840 - accuracy: 0.8596 - val_loss: 1.7303 - val_accuracy: 0.5195\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.3305 - accuracy: 0.8770 - val_loss: 1.7412 - val_accuracy: 0.5195\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.3503 - accuracy: 0.8675 - val_loss: 1.6449 - val_accuracy: 0.5195\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.3074 - accuracy: 0.8849 - val_loss: 1.2042 - val_accuracy: 0.5216\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.2663 - accuracy: 0.9001 - val_loss: 1.2120 - val_accuracy: 0.5260\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.2607 - accuracy: 0.8989 - val_loss: 2.4910 - val_accuracy: 0.5195\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.2423 - accuracy: 0.8996 - val_loss: 0.8196 - val_accuracy: 0.5866\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.2365 - accuracy: 0.9113 - val_loss: 1.4642 - val_accuracy: 0.5433\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 32s 1s/step - loss: 0.2157 - accuracy: 0.9180 - val_loss: 1.2499 - val_accuracy: 0.5758\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.2220 - accuracy: 0.9197 - val_loss: 1.2918 - val_accuracy: 0.6255\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 35s 1s/step - loss: 0.2366 - accuracy: 0.9169 - val_loss: 0.5046 - val_accuracy: 0.7727\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 41s 1s/step - loss: 0.1799 - accuracy: 0.9315 - val_loss: 0.2941 - val_accuracy: 0.8961\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 42s 1s/step - loss: 0.1811 - accuracy: 0.9281 - val_loss: 0.7187 - val_accuracy: 0.7273\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 42s 1s/step - loss: 0.1813 - accuracy: 0.9287 - val_loss: 0.2242 - val_accuracy: 0.9156\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.1537 - accuracy: 0.9439 - val_loss: 0.1573 - val_accuracy: 0.9307\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.1584 - accuracy: 0.9433 - val_loss: 0.1350 - val_accuracy: 0.9481\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 37s 1s/step - loss: 0.1353 - accuracy: 0.9495 - val_loss: 0.1177 - val_accuracy: 0.9481\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 34s 1s/step - loss: 0.1302 - accuracy: 0.9545 - val_loss: 0.1061 - val_accuracy: 0.9654\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 39s 1s/step - loss: 0.1231 - accuracy: 0.9500 - val_loss: 0.2315 - val_accuracy: 0.9264\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 43s 2s/step - loss: 0.1054 - accuracy: 0.9568 - val_loss: 0.0879 - val_accuracy: 0.9632\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 43s 2s/step - loss: 0.1255 - accuracy: 0.9565 - val_loss: 0.1486 - val_accuracy: 0.9416\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 53s 2s/step - loss: 0.1159 - accuracy: 0.9562 - val_loss: 0.4329 - val_accuracy: 0.8896\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 50s 2s/step - loss: 0.1128 - accuracy: 0.9590 - val_loss: 0.1284 - val_accuracy: 0.9545\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.1016 - accuracy: 0.9579 - val_loss: 0.1210 - val_accuracy: 0.9610\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.1235 - accuracy: 0.9528 - val_loss: 0.3236 - val_accuracy: 0.8853\n",
      "Epoch 29/100\n",
      " 7/28 [======>.......................] - ETA: 25s - loss: 0.0881 - accuracy: 0.9665"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "img_dims = (96,96,3)\n",
    "\n",
    "model = build(width=img_dims[0], height=img_dims[1], depth=img_dims[2],\n",
    "                            classes=2)\n",
    "\n",
    "opt = Adam(lr=1e-3, decay=1e-3/epochs)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=batch_size),\n",
    "                        validation_data=(testX,testY),\n",
    "                        steps_per_epoch=len(trainX) // batch_size,\n",
    "                        epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4298b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('gender_detection.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c065c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
